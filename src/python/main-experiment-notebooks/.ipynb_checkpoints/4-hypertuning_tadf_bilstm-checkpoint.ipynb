{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. Install necessary packages\n",
        "\n"
      ],
      "metadata": {
        "id": "kZmIZEMYIi_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjnMRrkODJuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ff9949-f245-4b79-ccd9-da2083b7c056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import necessary packages"
      ],
      "metadata": {
        "id": "7Jo9MGAGLfqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Conv2D, MaxPooling2D, SimpleRNN\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from keras.utils import to_categorical\n",
        "from keras_tuner import RandomSearch\n",
        "import keras"
      ],
      "metadata": {
        "id": "gKjw9oTgDVPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. If using colab complete these to import datasets from drive."
      ],
      "metadata": {
        "id": "IoXYlMk7JDAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "metadata": {
        "id": "FzJqxgFiDU-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1CjJyTJbyIs3rvtQh1cG0RyoKX_1inXb2/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/1HObEZNhDxOCCFcVnRQzW9y3w4nd_yiDx/view?usp=drive_link\n",
        "#https://drive.google.com/file/d/156TEVK1L99FXA7FFfMWle-Y4nvxhfOot/view?usp=drive_link\n",
        "your_file_all = drive.CreateFile({'id':'1CjJyTJbyIs3rvtQh1cG0RyoKX_1inXb2'})\n",
        "your_file_ta = drive.CreateFile({'id':'1HObEZNhDxOCCFcVnRQzW9y3w4nd_yiDx'})\n",
        "your_file_oc = drive.CreateFile({'id':'156TEVK1L99FXA7FFfMWle-Y4nvxhfOot'})"
      ],
      "metadata": {
        "id": "dWoopK4jDVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "your_file_all.GetContentFile('bitcoin-all-on-chain-and-technical-indicators.csv')\n",
        "your_file_ta.GetContentFile('bitcoin-all-technical-indicators.csv')\n",
        "your_file_oc.GetContentFile('bitcoin-all-on-chain.csv')\n",
        "ta_df = pd.read_csv('bitcoin-all-technical-indicators.csv')\n",
        "ta_df['timestamp'] = pd.to_datetime(ta_df['timestamp'])\n",
        "oc_df = pd.read_csv('bitcoin-all-on-chain.csv')\n",
        "oc_df['timestamp'] = pd.to_datetime(oc_df['timestamp'])\n",
        "oc_ta_df = pd.read_csv('bitcoin-all-on-chain-and-technical-indicators.csv')\n",
        "oc_ta_df['timestamp'] = pd.to_datetime(oc_ta_df['timestamp'])\n",
        "price_only_df = ta_df[['timestamp', 'price']]"
      ],
      "metadata": {
        "id": "JSXfj1ugDVMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. If running locally complete this to import datasets"
      ],
      "metadata": {
        "id": "nCBwmf31JTMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df = pd.read_csv('bitcoin-all-technical-indicators.csv')\n",
        "ta_df['timestamp'] = pd.to_datetime(ta_df['timestamp'])\n",
        "oc_df = pd.read_csv('bitcoin-all-on-chain.csv')\n",
        "oc_df['timestamp'] = pd.to_datetime(oc_df['timestamp'])\n",
        "oc_ta_df = pd.read_csv('bitcoin-all-on-chain-and-technical-indicators.csv')\n",
        "oc_ta_df['timestamp'] = pd.to_datetime(oc_ta_df['timestamp'])\n",
        "price_only_df = ta_df[['timestamp', 'price']]"
      ],
      "metadata": {
        "id": "41vTgAyVDVRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Function to plot two or more timeseries"
      ],
      "metadata": {
        "id": "vQ5tF_32NK2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_values(timestamp, *args):\n",
        "    import matplotlib\n",
        "\n",
        "    if len(args) % 2 != 0:\n",
        "        raise ValueError(\"Every feature should have a corresponding name\")\n",
        "\n",
        "    # Use a predefined style\n",
        "    plt.style.use('ggplot')\n",
        "\n",
        "    # Set figure size\n",
        "    fig, ax = plt.subplots(figsize=(12,6))\n",
        "\n",
        "    # Create a colormap that will generate colors\n",
        "    colormap = matplotlib.colormaps['tab10']\n",
        "\n",
        "    # Plot data with customized line for each feature\n",
        "    for i in range(0, len(args), 2):\n",
        "        feature_name = args[i]\n",
        "        feature_values = args[i+1]\n",
        "        color = colormap(i // 2 / (len(args)//2))\n",
        "        ax.plot(timestamp, feature_values, label=feature_name, color=color, linewidth=1)\n",
        "\n",
        "    # Set labels with improved readability\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.yaxis.set_label_position(\"right\") # This line moves y label to the right\n",
        "    ax.set_ylabel('Feature Values')\n",
        "    ax.yaxis.tick_right() # This line moves y axis to the right\n",
        "    ax.set_title('Features over Time')\n",
        "\n",
        "    # Format the timestamps and set locator\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # format as year-month-day\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3)) # display a label at the start of every 3rd month\n",
        "\n",
        "    # Rotate x-axis labels for better visibility\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Add a grid\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend()\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t_897CNsF-zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Function to scale the data and split it for training and testing"
      ],
      "metadata": {
        "id": "OEVLNHHbNUUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preperation(df, lookback, future, scale, test_size=0.1):\n",
        "    # Convert 'Date' column to datetime\n",
        "    try:\n",
        "        date_train = pd.to_datetime(df['timestamp'])\n",
        "        df = df.drop(columns=['timestamp'])\n",
        "    except:\n",
        "        date_train = [0]\n",
        "\n",
        "\n",
        "    # Ensure all data is float type\n",
        "    df = df.astype(float)\n",
        "\n",
        "    # Split data into training and testing before scaling\n",
        "    df_train, df_test = train_test_split(df, test_size=test_size, shuffle=False)\n",
        "\n",
        "    # Scale data\n",
        "    df_train_scaled = scale.fit_transform(df_train)\n",
        "    df_test_scaled = scale.transform(df_test)  # use the scaler fitted on the training data\n",
        "\n",
        "    # Create the feature and target arrays\n",
        "    X_train, y_train = [], []\n",
        "    for i in range(lookback, len(df_train_scaled)-future+1):\n",
        "        X_train.append(df_train_scaled[i-lookback:i, :])\n",
        "        y_train.append(df_train_scaled[i+future-1:i+future, 0])\n",
        "\n",
        "    X_test, y_test = [], []\n",
        "    for i in range(lookback, len(df_test_scaled)-future+1):\n",
        "        X_test.append(df_test_scaled[i-lookback:i, :])\n",
        "        y_test.append(df_test_scaled[i+future-1:i+future, 0])\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, date_train"
      ],
      "metadata": {
        "id": "ox4ALWPPF-14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Function that defines the RNN"
      ],
      "metadata": {
        "id": "BK2soAvgOnB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn_model(hp, input_shape):\n",
        "    model = keras.models.Sequential(name=\"rnn\")\n",
        "\n",
        "    model.add(keras.layers.SimpleRNN(units=hp.Int('units_1', min_value=32, max_value=512, step=32), return_sequences=True, input_shape=input_shape))\n",
        "    model.add(keras.layers.Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(keras.layers.SimpleRNN(units=hp.Int('units_2', min_value=32, max_value=256, step=32), return_sequences=False))\n",
        "    model.add(keras.layers.Dense(hp.Int('units_3', min_value=10, max_value=100, step=10)))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EEt-vobMICyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Function that defines the Bidirectional RNN"
      ],
      "metadata": {
        "id": "Bhdfa_DNOuX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_birnn_model(hp, input_shape):\n",
        "    model = keras.models.Sequential(name=\"birnn\")\n",
        "\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(units=hp.Int('units_1', min_value=32, max_value=512, step=32), return_sequences=True), input_shape=input_shape))\n",
        "    model.add(keras.layers.Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(units=hp.Int('units_2', min_value=32, max_value=256, step=32), return_sequences=False)))\n",
        "    model.add(keras.layers.Dense(hp.Int('units_3', min_value=10, max_value=100, step=10)))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "v9Yt7IjWIG7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Function that defines the LSTM"
      ],
      "metadata": {
        "id": "jywDp-ngO4q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(hp, input_shape):\n",
        "    model = keras.models.Sequential(name=\"lstm\")\n",
        "\n",
        "    model.add(keras.layers.LSTM(units=hp.Int('units_1', min_value=224, max_value=512, step=32), return_sequences=True, input_shape=input_shape))\n",
        "    model.add(keras.layers.Dropout(0.2)\n",
        "    model.add(keras.layers.LSTM(units=hp.Int('units_2', min_value=32, max_value=112, step=32), return_sequences=False))\n",
        "    model.add(keras.layers.Dense(hp.Int('units_3', min_value=10, max_value=100, step=10)))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gRCwAL02GEAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Function that defines the Bidirectional LSTM"
      ],
      "metadata": {
        "id": "oESb29ObO-Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bilstm_model(hp, input_shape):\n",
        "    model = keras.models.Sequential(name=\"bilstm\")\n",
        "\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=hp.Int('units_1', min_value=224, max_value=512, step=32), return_sequences=True), input_shape=input_shape))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=hp.Int('units_2', min_value=32, max_value=112, step=32), return_sequences=False)))\n",
        "    model.add(keras.layers.Dense(hp.Int('units_3', min_value=10, max_value=100, step=10)))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.01, sampling='LOG'))\n",
        "    model.compile(loss='mean_squared_error', optimizer=adam)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0V9eoIAmGk6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Function for running the hyperperameter search\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w2hvsyy9PDiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hyperparameter_search(build_model_function, X_train, y_train):\n",
        "    tuner = RandomSearch(\n",
        "        build_model_function,\n",
        "        objective='val_loss',\n",
        "        max_trials=15,\n",
        "        executions_per_trial=3,\n",
        "        directory='my_dir',\n",
        "        project_name='helloworld')\n",
        "\n",
        "    tuner.search_space_summary()\n",
        "\n",
        "    tuner.search(X_train, y_train, epochs=15, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    tuner.results_summary()"
      ],
      "metadata": {
        "id": "J9v2oyvXoM5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Function for running the hyperperameter search on each data set and model\n"
      ],
      "metadata": {
        "id": "pNpy-FYqs7vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_hyperparameter_search_on_datasets_and_models(datasets, models):\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        X_train, X_test, y_train, y_test = dataset\n",
        "        input_shape = (X_train.shape[1], X_train.shape[2])  # Assuming X_train is 3D (batch_size, timesteps, features)\n",
        "        for model_name, build_model_function in models.items():\n",
        "            print(f\"Running hyperparameter search for {model_name} model on {dataset_name} dataset...\")\n",
        "            run_hyperparameter_search(lambda hp: build_model_function(hp, input_shape), X_train, y_train)\n"
      ],
      "metadata": {
        "id": "f3ubKhYZK0Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale_ta = MinMaxScaler(feature_range=(0,1))\n",
        "X_train_ta, X_test_ta, y_train_ta, y_test_ta, date_train = data_preperation(ta_df, 60, 1, scale_ta)\n",
        "\n",
        "scale_oc_ta = MinMaxScaler(feature_range=(0,1))\n",
        "X_train_oc_ta, X_test_oc_ta, y_train_oc_ta, y_test_oc_ta, date_train = data_preperation(oc_ta_df, 60, 1, scale_oc_ta)\n",
        "\n",
        "\n",
        "\n",
        "# Define your datasets\n",
        "datasets = {\n",
        "    'ta_df': (X_train_ta, X_test_ta, y_train_ta, y_test_ta),\n",
        "    #'oc_ta_df': (X_train_oc_ta, X_test_oc_ta, y_train_oc_ta, y_test_oc_ta)\n",
        "}\n",
        "\n",
        "# Define your models\n",
        "models = {\n",
        "    #'rnn': build_rnn_model,\n",
        "    #'birnn': build_birnn_model,\n",
        "    #'lstm': build_lstm_model,\n",
        "    'bilstm': build_bilstm_model\n",
        "}\n",
        "\n",
        "# Run the hyperparameter search\n",
        "run_hyperparameter_search_on_datasets_and_models(datasets, models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJZpqu2LWmYl",
        "outputId": "58205bea-f202-4dda-89ee-2d37ad18d1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 01m 57s]\n",
            "val_loss: 0.001811000828941663\n",
            "\n",
            "Best val_loss So Far: 0.0011915800860151649\n",
            "Total elapsed time: 00h 30m 04s\n",
            "Results summary\n",
            "Results in my_dir/helloworld\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units_1: 480\n",
            "units_2: 96\n",
            "units_3: 10\n",
            "learning_rate: 0.0012625225902271942\n",
            "Score: 0.0011915800860151649\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units_1: 288\n",
            "units_2: 64\n",
            "units_3: 20\n",
            "learning_rate: 0.0017825900024172885\n",
            "Score: 0.001268486026674509\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units_1: 480\n",
            "units_2: 32\n",
            "units_3: 40\n",
            "learning_rate: 0.0035072152287283025\n",
            "Score: 0.0013616127350057166\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units_1: 224\n",
            "units_2: 64\n",
            "units_3: 100\n",
            "learning_rate: 0.004430408635132608\n",
            "Score: 0.0014001492333287995\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units_1: 384\n",
            "units_2: 96\n",
            "units_3: 90\n",
            "learning_rate: 0.00012763229197396264\n",
            "Score: 0.0015230241309230526\n",
            "\n",
            "Trial 12 summary\n",
            "Hyperparameters:\n",
            "units_1: 384\n",
            "units_2: 64\n",
            "units_3: 20\n",
            "learning_rate: 0.00012474964863605313\n",
            "Score: 0.0015999926254153252\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units_1: 224\n",
            "units_2: 64\n",
            "units_3: 100\n",
            "learning_rate: 0.009458590727480821\n",
            "Score: 0.0016076767351478338\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units_1: 416\n",
            "units_2: 64\n",
            "units_3: 100\n",
            "learning_rate: 0.0006929396347746626\n",
            "Score: 0.001631600083783269\n",
            "\n",
            "Trial 11 summary\n",
            "Hyperparameters:\n",
            "units_1: 256\n",
            "units_2: 32\n",
            "units_3: 80\n",
            "learning_rate: 0.008128524746629794\n",
            "Score: 0.0016321934138735135\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units_1: 448\n",
            "units_2: 32\n",
            "units_3: 40\n",
            "learning_rate: 0.00016379559434627597\n",
            "Score: 0.0016478674563889701\n"
          ]
        }
      ]
    }
  ]
}